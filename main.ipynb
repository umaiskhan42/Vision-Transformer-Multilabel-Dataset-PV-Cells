{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12376,"status":"ok","timestamp":1727536565768,"user":{"displayName":"Solid Tech","userId":"07733259367386498051"},"user_tz":-300},"id":"d8N0rBkBGmDL","outputId":"3ffebc3d-f04c-4390-eb51-32b6986dd490"},"outputs":[],"source":["%pip install vit-keras\n","%pip install tensorflow-addons"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12698,"status":"ok","timestamp":1727536578460,"user":{"displayName":"Solid Tech","userId":"07733259367386498051"},"user_tz":-300},"id":"Fnq0pkPxGmDM","outputId":"c85aeddf-9c0a-46a7-eb20-73a94360b3f0"},"outputs":[],"source":["import tensorflow as tf\n","import keras\n","\n","print(f\"TensorFlow version: {tf.__version__}\")\n","print(f\"Keras version: {keras.__version__}\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1707,"status":"ok","timestamp":1727536580158,"user":{"displayName":"Solid Tech","userId":"07733259367386498051"},"user_tz":-300},"id":"iHpjuuZHGmDO"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import matplotlib.pyplot as plt\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n","import cv2\n","from sklearn import preprocessing\n","from pathlib import Path"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1727536580159,"user":{"displayName":"Solid Tech","userId":"07733259367386498051"},"user_tz":-300},"id":"Yf_W0y46GmDP"},"outputs":[],"source":["labels_train = []\n","labels_test = []\n","\n","path_train = []\n","path_test = []"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1727536580159,"user":{"displayName":"Solid Tech","userId":"07733259367386498051"},"user_tz":-300},"id":"9pGc0b8WGmDQ"},"outputs":[],"source":["train_path = r\"path-to-train-images\"\n","test_path = r\"path-to-test-images\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406},"executionInfo":{"elapsed":1832,"status":"ok","timestamp":1727537105307,"user":{"displayName":"Solid Tech","userId":"07733259367386498051"},"user_tz":-300},"id":"izqbPawHOfmF","outputId":"5344c263-c790-4658-a1e3-cd794fcfcb76"},"outputs":[],"source":["import cv2\n","import xml.etree.ElementTree as ET\n","import matplotlib.pyplot as plt\n","\n","# Function to parse the .xml file and get bounding box details\n","def parse_xml(xml_file):\n","    tree = ET.parse(xml_file)\n","    root = tree.getroot()\n","\n","    boxes = []\n","    for obj in root.findall('object'):\n","        label = obj.find('name').text\n","        bbox = obj.find('bndbox')\n","        xmin = int(bbox.find('xmin').text)\n","        ymin = int(bbox.find('ymin').text)\n","        xmax = int(bbox.find('xmax').text)\n","        ymax = int(bbox.find('ymax').text)\n","\n","        boxes.append((label, xmin, ymin, xmax, ymax))\n","    return boxes\n","\n","# Function to visualize the image with bounding boxes and labels\n","def visualize_image_with_boxes(image_path, boxes):\n","    image = cv2.imread(image_path)\n","\n","    for box in boxes:\n","        label, xmin, ymin, xmax, ymax = box\n","        # Draw rectangle around the object\n","        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n","        # Put the label near the object\n","        cv2.putText(image, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n","\n","    # Convert BGR image to RGB for displaying with matplotlib\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    # Display the image using matplotlib\n","    plt.imshow(image_rgb)\n","    plt.axis('off')\n","    plt.show()\n","\n","#Usage\n","xml_file = 'path here '  # Replace with your XML file path\n","image_file = 'path here'  # Replace with your image path\n","\n","# Parse XML and visualize\n","boxes = parse_xml(xml_file)\n","visualize_image_with_boxes(image_file, boxes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":278320,"status":"error","timestamp":1727536858472,"user":{"displayName":"Solid Tech","userId":"07733259367386498051"},"user_tz":-300},"id":"3eQyGTK3GmDT","outputId":"f93c05c7-40ae-4024-b19d-cf47759b154a"},"outputs":[],"source":["import xml.etree.ElementTree as ET\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","def extract_labels_from_xml(xml_file_path):\n","    # Parse the XML file and get the root of the XML document\n","    tree = ET.parse(xml_file_path)\n","    root = tree.getroot()\n","\n","    # Find the 'object' tags in the XML document\n","    objects = root.findall('object')\n","\n","    # Extract the labels from all 'object' tags\n","    labels = [obj.find('name').text for obj in objects]\n","\n","    return labels\n","\n","# Specify the path to the 'annotations' directory\n","annotations_dir = r\"/content/drive/MyDrive/Solar_PV/trainval/Annotations\"\n","labels_train = []\n","for filename in os.listdir(train_path):\n","    if filename.endswith(\".jpg\"):\n","        # Extract the labels from the corresponding XML file\n","        xml_file_path = os.path.join(annotations_dir, filename.replace('.jpg', '.xml'))\n","        labels = extract_labels_from_xml(xml_file_path)\n","\n","        labels_train.append(labels)\n","\n","        path_train.append(os.path.join(train_path, filename))\n","\n","# Apply MultiLabelBinarizer to the training labels\n","mlb = MultiLabelBinarizer()\n","labels_train_bin = mlb.fit_transform(labels_train)\n","# 0: Black core, 1: Finger\n","print(labels_train_bin)\n","print(mlb.classes_)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1727536858474,"user":{"displayName":"Solid Tech","userId":"07733259367386498051"},"user_tz":-300},"id":"YgR8fET0GmDU"},"outputs":[],"source":["print(len(mlb.classes_))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HgH6b6-tGmDW","outputId":"fe9ebde6-fe13-4769-eb11-e596a06b6006"},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","# Load and preprocess images\n","def load_and_preprocess_images(image_paths):\n","    images = []\n","    for img_path in image_paths:\n","        img = cv2.imread(img_path)  # Load image using OpenCV\n","        img = cv2.resize(img, (desired_width, desired_height))  # Resize image\n","        img = img.astype(np.float32) / 255.0  # Normalize pixel values\n","        images.append(img)\n","    return np.array(images)\n","\n","# Define desired width and height for resizing images\n","desired_width = 224\n","desired_height = 224\n","\n","# Load and preprocess images\n","x_train = load_and_preprocess_images(path_train)\n","\n","# Split data into training and validation sets\n","x_train, x_val, y_train, y_val = train_test_split(x_train, labels_train_bin, test_size=0.1, random_state=42)\n","\n","print(\"x_train shape:\", x_train.shape)\n","print(\"y_train shape:\", y_train.shape)\n","print(\"x_val shape:\", x_val.shape)\n","print(\"y_val shape:\", y_val.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Wz37TxdwGmDX"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from PIL import Image\n","import numpy as np\n","from xml.etree import ElementTree as ET\n","def extract_boxes(filename):\n","    # load and parse the file\n","    tree = ET.parse(filename)\n","    # get the root of the document\n","    root = tree.getroot()\n","    # extract each bounding box\n","    boxes = list()\n","    names = list()\n","    for object in root.findall('.//object'):\n","        name = object.find('name').text\n","        names.append(name)\n","        box = object.find('bndbox')\n","        xmin = int(box.find('xmin').text)\n","        ymin = int(box.find('ymin').text)\n","        xmax = int(box.find('xmax').text)\n","        ymax = int(box.find('ymax').text)\n","        coors = [xmin, ymin, xmax, ymax]\n","        boxes.append(coors)\n","    return boxes, names"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HX-gB44XGmDY","outputId":"c1c77695-86b0-4b8b-9d63-0b9934450494"},"outputs":[],"source":["# visualize train image with multilabels\n","\n","# Load image\n","from PIL import Image\n","im = np.array(Image.open(r'/kaggle/input/solar-cell-el-image/solar_cell_EL_image/PVELAD/EL2021/trainval/JPEGImages/img000005.jpg'))\n","# Create figure and axes\n","fig,ax = plt.subplots(1)\n","\n","# Display the image\n","ax.imshow(im)\n","\n","# Extract bounding boxes and object names from the corresponding XML file\n","boxes, names = extract_boxes(r'/kaggle/input/solar-cell-el-image/solar_cell_EL_image/PVELAD/EL2021/trainval/Annotations/img000005.xml')\n","\n","# Create a Rectangle patch for each bounding box and add it to the plot\n","for box, name in zip(boxes, names):\n","    rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],linewidth=1,edgecolor='r',facecolor='none')\n","    ax.add_patch(rect)\n","    plt.text(box[0], box[1], name, color='r')\n","\n","# Show the plot with bounding boxes\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HtHI3iqKGmDY"},"outputs":[],"source":["# from vit_keras import vit\n","\n","# # Load the pre-trained Vision Transformer model\n","# vit_model = vit.vit_b16(\n","#     image_size=224,\n","#     activation='softmax',\n","#     pretrained=False,\n","#     include_top=True,\n","#     pretrained_top=False,\n","#     classes=12\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mDoTVGCn9Y9Z"},"outputs":[],"source":["# import tensorflow as tf\n","# from vit_keras import vit\n","# from tensorflow.keras.applications import MobileNet, EfficientNetB0, ResNet50\n","\n","# # Load the pre-trained Vision Transformer model\n","# vit_model = vit.vit_b16(\n","#     image_size=224,\n","#     activation='softmax',\n","#     pretrained=False,\n","#     include_top=True,\n","#     pretrained_top=False,\n","#     classes=12\n","# )\n","\n","# # Load the pre-trained MobileNet model\n","# mobilenet_model = MobileNet(\n","#     input_shape=(224, 224, 3),\n","#     alpha=1.0,\n","#     depth_multiplier=1,\n","#     dropout=0.001,\n","#     include_top=True,\n","#     weights=None,\n","#     classes=12\n","# )\n","\n","# # Load the pre-trained EfficientNet model\n","# efficientnet_model = EfficientNetB0(\n","#     input_shape=(224, 224, 3),\n","#     include_top=True,\n","#     weights=None,\n","#     classes=12\n","# )\n","\n","# # Load the pre-trained ResNet model\n","# resnet_model = ResNet50(\n","#     input_shape=(224, 224, 3),\n","#     include_top=True,\n","#     weights=None,\n","#     classes=12\n","# )\n","\n","# # Print summaries of the models\n","# print(\"Vision Transformer Model Summary:\")\n","# vit_model.summary()\n","\n","# print(\"\\nMobileNet Model Summary:\")\n","# mobilenet_model.summary()\n","\n","# print(\"\\nEfficientNet Model Summary:\")\n","# efficientnet_model.summary()\n","\n","# print(\"\\nResNet Model Summary:\")\n","# resnet_model.summary()\n","\n","# # Note: If your dataset is multi-label, ensure you are using appropriate loss functions and metrics when compiling the models.\n"]},{"cell_type":"markdown","metadata":{},"source":["Model Definitions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29609,"status":"ok","timestamp":1720451787479,"user":{"displayName":"Solid Tech","userId":"07733259367386498051"},"user_tz":-300},"id":"Y08Pfjuc99s3","outputId":"c68b3115-530f-43ee-fbf8-0bfa2ec0a600"},"outputs":[],"source":["import tensorflow as tf\n","from vit_keras import vit\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.applications import MobileNet, EfficientNetB0, ResNet50\n","\n","# Load the pre-trained Vision Transformer model\n","vit_model = vit.vit_b16(\n","    image_size=224,\n","    activation='softmax',\n","    pretrained=False,\n","    include_top=True,\n","    pretrained_top=False,\n","    classes=12\n",")\n","\n","# Load the pre-trained MobileNet model\n","mobilenet_base = MobileNet(\n","    input_shape=(224, 224, 3),\n","    include_top=False,\n","    weights=None\n",")\n","x = mobilenet_base.output\n","x = tf.keras.layers.GlobalAveragePooling2D()(x)\n","x = Dense(12, activation='softmax')(x)\n","mobilenet_model = Model(inputs=mobilenet_base.input, outputs=x)\n","\n","# Load the pre-trained EfficientNet model\n","efficientnet_base = EfficientNetB0(\n","    input_shape=(224, 224, 3),\n","    include_top=False,\n","    weights=None\n",")\n","x = efficientnet_base.output\n","x = tf.keras.layers.GlobalAveragePooling2D()(x)\n","x = Dense(12, activation='softmax')(x)\n","efficientnet_model = Model(inputs=efficientnet_base.input, outputs=x)\n","\n","# Load the pre-trained ResNet model\n","resnet_base = ResNet50(\n","    input_shape=(224, 224, 3),\n","    include_top=False,\n","    weights=None\n",")\n","x = resnet_base.output\n","x = tf.keras.layers.GlobalAveragePooling2D()(x)\n","x = Dense(12, activation='softmax')(x)\n","resnet_model = Model(inputs=resnet_base.input, outputs=x)\n","\n","# Print summaries of the models\n","print(\"Vision Transformer Model Summary:\")\n","vit_model.summary()\n","\n","print(\"\\nMobileNet Model Summary:\")\n","mobilenet_model.summary()\n","\n","print(\"\\nEfficientNet Model Summary:\")\n","efficientnet_model.summary()\n","\n","print(\"\\nResNet Model Summary:\")\n","resnet_model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKQ4Aom9GmDa"},"outputs":[],"source":["# from tensorflow.keras.optimizers import Adam\n","\n","# # Compile the model with binary crossentropy loss\n","# optimizer = Adam(learning_rate=0.0001)\n","# vit_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKMAQRB6hQhF"},"outputs":[],"source":["# from tensorflow.keras.optimizers import Adam\n","\n","# # Compile the model with binary crossentropy loss\n","# optimizer = Adam(learning_rate=0.0001)\n","# mobilenet_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OaqpgBDPGmDa"},"outputs":[],"source":["# vit_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RWuql4pvGmDa"},"outputs":[],"source":["# import tensorflow as tf\n","\n","# # Check if TensorFlow can access GPUs\n","# gpu_devices = tf.config.list_physical_devices('GPU')\n","\n","# if gpu_devices:\n","#     print(\"Found GPU(s):\")\n","#     for gpu in gpu_devices:\n","#         print(gpu)\n","# else:\n","#     print(\"No GPU(s) found. TensorFlow will use CPU.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wuajMtBvGmDb"},"outputs":[],"source":["# !nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-RCl0epGmDc"},"outputs":[],"source":["# with tf.device('/gpu:0'):\n","#     history = vit_model.fit(x_train, y_train, epochs=2, batch_size=8, validation_data=(x_val, y_val))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zP9CIVcPGmDf"},"outputs":[],"source":["# history = mobilenet_model.fit(x_train, y_train, epochs=50, batch_size=8, validation_data=(x_val, y_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J19mkUYsH9pH"},"outputs":[],"source":["# # Assuming 'model' is your trained TensorFlow model\n","# vit_model.save('/content/drive/MyDrive/Solar_PV')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dOaWjQI7ISyO"},"outputs":[],"source":["# # Assuming 'model' is your trained TensorFlow model\n","# mobilenet_model.save_weights('/content/drive/MyDrive/Solar_PV/mnet_weights.h5')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lkxSnCfGmDf"},"outputs":[],"source":["# plt.plot(history.history['accuracy'], label='train', color=\"red\")\n","# plt.plot(history.history['val_accuracy'], label='validation', color=\"blue\")\n","# plt.title('Model accuracy')\n","# plt.legend(loc='upper left')\n","# plt.ylabel('accuracy')\n","# plt.xlabel('epoch')\n","# plt.show()\n","\n","# plt.plot(history.history['loss'], label='train', color=\"red\")\n","# plt.plot(history.history['val_loss'], label='validation', color=\"blue\")\n","# plt.title('Model loss')\n","# plt.legend(loc='upper left')\n","# plt.xlabel('epoch')\n","# plt.ylabel('loss')\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92aoo4UCGmDg"},"outputs":[],"source":["# pred = vit_model.predict(x_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qFEvaQDnGmDg"},"outputs":[],"source":["# # Make predictions on the validation data\n","# y_pred = vit_model.predict(x_val)\n","\n","# # Convert probabilities to binary predictions\n","# y_pred_binary = (y_pred > 0.5).astype(int)\n","\n","# # Display results\n","# for i in range(len(y_val)):\n","#     print(\"Sample\", i+1)\n","#     print(\"Actual Labels:\", mlb.inverse_transform(np.array([y_val[i]])))\n","#     print(\"Predicted Labels:\", mlb.inverse_transform(np.array([y_pred_binary[i]])))\n","#     print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3WyvLh9TGmDh"},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","\n","# # Function to display image with predicted labels\n","# def display_image_with_labels(image, actual_labels, predicted_labels):\n","#     plt.figure(figsize=(8, 8))\n","#     plt.imshow(image)\n","#     plt.axis('off')\n","\n","#     # Add actual labels\n","#     actual_label_str = ', '.join(actual_labels[0])\n","#     plt.text(10, 20, f'Actual: {actual_label_str}', color='white', fontsize=10)\n","\n","#     # Add predicted labels\n","#     predicted_label_str = ', '.join(predicted_labels[0])\n","#     plt.text(10, 40, f'Predicted: {predicted_label_str}', color='white', fontsize=10)\n","\n","#     plt.show()\n","\n","# # Display a sample image with its predicted labels\n","# sample_index = 0  # Index of the sample in the validation set\n","# sample_image = x_val[sample_index]\n","# actual_labels = mlb.inverse_transform(np.array([y_val[sample_index]]))\n","# predicted_labels = mlb.inverse_transform(np.array([y_pred_binary[sample_index]]))\n","\n","# display_image_with_labels(sample_image, actual_labels, predicted_labels)\n"]},{"cell_type":"markdown","metadata":{"id":"_9gEk7Ypq6UC"},"source":["Model2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTX4zXxkq7ZG"},"outputs":[],"source":["# from tensorflow.keras.optimizers import Adam\n","\n","# # Compile the model with binary crossentropy loss\n","# optimizer = Adam(learning_rate=0.0001)\n","# efficientnet_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","# history = efficientnet_model.fit(x_train, y_train, epochs=50, batch_size=8, validation_data=(x_val, y_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"lJARnHv_rFqv","outputId":"633c8f02-bca8-4a5f-ae36-73c93bf80ffb"},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam\n","\n","# Compile the model with binary crossentropy loss\n","optimizer = Adam(learning_rate=0.0001)\n","resnet_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","history =resnet_model.fit(x_train, y_train, epochs=10, batch_size=8, validation_data=(x_val, y_val))"]}],"metadata":{"colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4958474,"sourceId":8346804,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
